{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.9784172661870505,
  "eval_steps": 500,
  "global_step": 1040,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.047961630695443645,
      "grad_norm": 0.8111532330513,
      "learning_rate": 4.95673076923077e-05,
      "loss": 16.6155,
      "step": 10
    },
    {
      "epoch": 0.09592326139088729,
      "grad_norm": 1.1074323654174805,
      "learning_rate": 4.9086538461538464e-05,
      "loss": 16.4626,
      "step": 20
    },
    {
      "epoch": 0.14388489208633093,
      "grad_norm": 1.6210243701934814,
      "learning_rate": 4.860576923076923e-05,
      "loss": 16.3452,
      "step": 30
    },
    {
      "epoch": 0.19184652278177458,
      "grad_norm": 1.60557222366333,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 16.1507,
      "step": 40
    },
    {
      "epoch": 0.23980815347721823,
      "grad_norm": 2.0622525215148926,
      "learning_rate": 4.764423076923077e-05,
      "loss": 15.8891,
      "step": 50
    },
    {
      "epoch": 0.28776978417266186,
      "grad_norm": 2.525348663330078,
      "learning_rate": 4.716346153846154e-05,
      "loss": 15.5783,
      "step": 60
    },
    {
      "epoch": 0.33573141486810554,
      "grad_norm": 2.613140821456909,
      "learning_rate": 4.668269230769231e-05,
      "loss": 15.2422,
      "step": 70
    },
    {
      "epoch": 0.38369304556354916,
      "grad_norm": 3.011113166809082,
      "learning_rate": 4.620192307692308e-05,
      "loss": 14.7616,
      "step": 80
    },
    {
      "epoch": 0.4316546762589928,
      "grad_norm": 4.079647064208984,
      "learning_rate": 4.5721153846153846e-05,
      "loss": 14.2,
      "step": 90
    },
    {
      "epoch": 0.47961630695443647,
      "grad_norm": 4.9369401931762695,
      "learning_rate": 4.524038461538462e-05,
      "loss": 13.4315,
      "step": 100
    },
    {
      "epoch": 0.5275779376498801,
      "grad_norm": 6.754444122314453,
      "learning_rate": 4.4759615384615386e-05,
      "loss": 12.3116,
      "step": 110
    },
    {
      "epoch": 0.5755395683453237,
      "grad_norm": 8.355316162109375,
      "learning_rate": 4.427884615384615e-05,
      "loss": 10.6074,
      "step": 120
    },
    {
      "epoch": 0.6235011990407674,
      "grad_norm": 8.299802780151367,
      "learning_rate": 4.379807692307693e-05,
      "loss": 8.7222,
      "step": 130
    },
    {
      "epoch": 0.6714628297362111,
      "grad_norm": 7.7861456871032715,
      "learning_rate": 4.3317307692307694e-05,
      "loss": 7.2833,
      "step": 140
    },
    {
      "epoch": 0.7194244604316546,
      "grad_norm": 5.457474231719971,
      "learning_rate": 4.283653846153846e-05,
      "loss": 6.0044,
      "step": 150
    },
    {
      "epoch": 0.7673860911270983,
      "grad_norm": 3.560922384262085,
      "learning_rate": 4.2355769230769234e-05,
      "loss": 5.0648,
      "step": 160
    },
    {
      "epoch": 0.815347721822542,
      "grad_norm": 2.8596432209014893,
      "learning_rate": 4.1875e-05,
      "loss": 4.5183,
      "step": 170
    },
    {
      "epoch": 0.8633093525179856,
      "grad_norm": 2.43196964263916,
      "learning_rate": 4.139423076923077e-05,
      "loss": 4.1001,
      "step": 180
    },
    {
      "epoch": 0.9112709832134293,
      "grad_norm": 2.1432912349700928,
      "learning_rate": 4.091346153846154e-05,
      "loss": 3.7567,
      "step": 190
    },
    {
      "epoch": 0.9592326139088729,
      "grad_norm": 1.9591917991638184,
      "learning_rate": 4.043269230769231e-05,
      "loss": 3.4412,
      "step": 200
    },
    {
      "epoch": 1.0047961630695443,
      "grad_norm": 1.794302225112915,
      "learning_rate": 3.995192307692308e-05,
      "loss": 2.9987,
      "step": 210
    },
    {
      "epoch": 1.052757793764988,
      "grad_norm": 1.662681221961975,
      "learning_rate": 3.947115384615385e-05,
      "loss": 2.9089,
      "step": 220
    },
    {
      "epoch": 1.1007194244604317,
      "grad_norm": 1.504362940788269,
      "learning_rate": 3.8990384615384617e-05,
      "loss": 2.6931,
      "step": 230
    },
    {
      "epoch": 1.1486810551558753,
      "grad_norm": 1.426518440246582,
      "learning_rate": 3.850961538461539e-05,
      "loss": 2.5017,
      "step": 240
    },
    {
      "epoch": 1.196642685851319,
      "grad_norm": 1.2748287916183472,
      "learning_rate": 3.802884615384616e-05,
      "loss": 2.3273,
      "step": 250
    },
    {
      "epoch": 1.2446043165467626,
      "grad_norm": 1.199715256690979,
      "learning_rate": 3.7548076923076924e-05,
      "loss": 2.1753,
      "step": 260
    },
    {
      "epoch": 1.2925659472422062,
      "grad_norm": 1.1283795833587646,
      "learning_rate": 3.70673076923077e-05,
      "loss": 2.0372,
      "step": 270
    },
    {
      "epoch": 1.34052757793765,
      "grad_norm": 1.0445195436477661,
      "learning_rate": 3.658653846153846e-05,
      "loss": 1.9294,
      "step": 280
    },
    {
      "epoch": 1.3884892086330936,
      "grad_norm": 1.0332285165786743,
      "learning_rate": 3.610576923076923e-05,
      "loss": 1.8269,
      "step": 290
    },
    {
      "epoch": 1.4364508393285371,
      "grad_norm": 0.9824855327606201,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 1.7313,
      "step": 300
    },
    {
      "epoch": 1.484412470023981,
      "grad_norm": 0.9675929546356201,
      "learning_rate": 3.5144230769230766e-05,
      "loss": 1.661,
      "step": 310
    },
    {
      "epoch": 1.5323741007194245,
      "grad_norm": 0.8624894618988037,
      "learning_rate": 3.466346153846154e-05,
      "loss": 1.5973,
      "step": 320
    },
    {
      "epoch": 1.580335731414868,
      "grad_norm": 1.0047152042388916,
      "learning_rate": 3.418269230769231e-05,
      "loss": 1.5069,
      "step": 330
    },
    {
      "epoch": 1.6282973621103118,
      "grad_norm": 0.9708333015441895,
      "learning_rate": 3.370192307692308e-05,
      "loss": 1.4622,
      "step": 340
    },
    {
      "epoch": 1.6762589928057554,
      "grad_norm": 0.8738510608673096,
      "learning_rate": 3.322115384615385e-05,
      "loss": 1.4302,
      "step": 350
    },
    {
      "epoch": 1.724220623501199,
      "grad_norm": 0.8541231751441956,
      "learning_rate": 3.274038461538462e-05,
      "loss": 1.3551,
      "step": 360
    },
    {
      "epoch": 1.7721822541966428,
      "grad_norm": 0.8546605110168457,
      "learning_rate": 3.225961538461539e-05,
      "loss": 1.3332,
      "step": 370
    },
    {
      "epoch": 1.8201438848920863,
      "grad_norm": 0.8996918797492981,
      "learning_rate": 3.1778846153846154e-05,
      "loss": 1.2962,
      "step": 380
    },
    {
      "epoch": 1.86810551558753,
      "grad_norm": 0.8039852380752563,
      "learning_rate": 3.129807692307692e-05,
      "loss": 1.2595,
      "step": 390
    },
    {
      "epoch": 1.9160671462829737,
      "grad_norm": 0.840606689453125,
      "learning_rate": 3.0817307692307695e-05,
      "loss": 1.2273,
      "step": 400
    },
    {
      "epoch": 1.9640287769784173,
      "grad_norm": 0.7938670516014099,
      "learning_rate": 3.0336538461538462e-05,
      "loss": 1.2061,
      "step": 410
    },
    {
      "epoch": 2.0095923261390887,
      "grad_norm": 0.8380715847015381,
      "learning_rate": 2.9855769230769232e-05,
      "loss": 1.127,
      "step": 420
    },
    {
      "epoch": 2.0575539568345325,
      "grad_norm": 0.7916219234466553,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 1.1533,
      "step": 430
    },
    {
      "epoch": 2.105515587529976,
      "grad_norm": 0.8338707089424133,
      "learning_rate": 2.889423076923077e-05,
      "loss": 1.1268,
      "step": 440
    },
    {
      "epoch": 2.1534772182254196,
      "grad_norm": 0.8040385246276855,
      "learning_rate": 2.841346153846154e-05,
      "loss": 1.1217,
      "step": 450
    },
    {
      "epoch": 2.2014388489208634,
      "grad_norm": 0.785857617855072,
      "learning_rate": 2.793269230769231e-05,
      "loss": 1.071,
      "step": 460
    },
    {
      "epoch": 2.249400479616307,
      "grad_norm": 0.8035362958908081,
      "learning_rate": 2.745192307692308e-05,
      "loss": 1.0703,
      "step": 470
    },
    {
      "epoch": 2.2973621103117505,
      "grad_norm": 0.7828649878501892,
      "learning_rate": 2.6971153846153847e-05,
      "loss": 1.0692,
      "step": 480
    },
    {
      "epoch": 2.3453237410071943,
      "grad_norm": 0.9080012440681458,
      "learning_rate": 2.6490384615384618e-05,
      "loss": 1.0412,
      "step": 490
    },
    {
      "epoch": 2.393285371702638,
      "grad_norm": 0.7747740149497986,
      "learning_rate": 2.6009615384615388e-05,
      "loss": 1.0422,
      "step": 500
    },
    {
      "epoch": 2.4412470023980815,
      "grad_norm": 0.66731858253479,
      "learning_rate": 2.5528846153846155e-05,
      "loss": 1.02,
      "step": 510
    },
    {
      "epoch": 2.4892086330935252,
      "grad_norm": 1.1498442888259888,
      "learning_rate": 2.5048076923076925e-05,
      "loss": 1.0153,
      "step": 520
    },
    {
      "epoch": 2.537170263788969,
      "grad_norm": 0.8092750310897827,
      "learning_rate": 2.4567307692307692e-05,
      "loss": 0.9953,
      "step": 530
    },
    {
      "epoch": 2.5851318944844124,
      "grad_norm": 0.8137931823730469,
      "learning_rate": 2.4086538461538462e-05,
      "loss": 0.9853,
      "step": 540
    },
    {
      "epoch": 2.633093525179856,
      "grad_norm": 0.8037379384040833,
      "learning_rate": 2.3605769230769233e-05,
      "loss": 0.985,
      "step": 550
    },
    {
      "epoch": 2.6810551558753,
      "grad_norm": 0.7136821150779724,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.9631,
      "step": 560
    },
    {
      "epoch": 2.7290167865707433,
      "grad_norm": 0.7345108389854431,
      "learning_rate": 2.264423076923077e-05,
      "loss": 0.9614,
      "step": 570
    },
    {
      "epoch": 2.776978417266187,
      "grad_norm": 0.678331732749939,
      "learning_rate": 2.2163461538461537e-05,
      "loss": 0.9606,
      "step": 580
    },
    {
      "epoch": 2.824940047961631,
      "grad_norm": 0.7948895692825317,
      "learning_rate": 2.168269230769231e-05,
      "loss": 0.9484,
      "step": 590
    },
    {
      "epoch": 2.8729016786570742,
      "grad_norm": 0.7715781331062317,
      "learning_rate": 2.1201923076923078e-05,
      "loss": 0.9294,
      "step": 600
    },
    {
      "epoch": 2.920863309352518,
      "grad_norm": 0.7295248508453369,
      "learning_rate": 2.0721153846153848e-05,
      "loss": 0.9241,
      "step": 610
    },
    {
      "epoch": 2.968824940047962,
      "grad_norm": 0.6605623960494995,
      "learning_rate": 2.0240384615384618e-05,
      "loss": 0.9217,
      "step": 620
    },
    {
      "epoch": 3.014388489208633,
      "grad_norm": 0.7887304425239563,
      "learning_rate": 1.9759615384615385e-05,
      "loss": 0.879,
      "step": 630
    },
    {
      "epoch": 3.062350119904077,
      "grad_norm": 0.7666914463043213,
      "learning_rate": 1.9278846153846155e-05,
      "loss": 0.902,
      "step": 640
    },
    {
      "epoch": 3.1103117505995206,
      "grad_norm": 0.8659685254096985,
      "learning_rate": 1.8798076923076922e-05,
      "loss": 0.9153,
      "step": 650
    },
    {
      "epoch": 3.158273381294964,
      "grad_norm": 0.8001641631126404,
      "learning_rate": 1.8317307692307693e-05,
      "loss": 0.9026,
      "step": 660
    },
    {
      "epoch": 3.2062350119904077,
      "grad_norm": 0.7825090289115906,
      "learning_rate": 1.7836538461538463e-05,
      "loss": 0.9113,
      "step": 670
    },
    {
      "epoch": 3.2541966426858515,
      "grad_norm": 0.6043582558631897,
      "learning_rate": 1.735576923076923e-05,
      "loss": 0.8962,
      "step": 680
    },
    {
      "epoch": 3.302158273381295,
      "grad_norm": 0.697430431842804,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 0.9045,
      "step": 690
    },
    {
      "epoch": 3.3501199040767387,
      "grad_norm": 0.8022395372390747,
      "learning_rate": 1.639423076923077e-05,
      "loss": 0.892,
      "step": 700
    },
    {
      "epoch": 3.3980815347721824,
      "grad_norm": 0.6920307278633118,
      "learning_rate": 1.5913461538461537e-05,
      "loss": 0.8753,
      "step": 710
    },
    {
      "epoch": 3.446043165467626,
      "grad_norm": 0.8015065789222717,
      "learning_rate": 1.5432692307692308e-05,
      "loss": 0.8692,
      "step": 720
    },
    {
      "epoch": 3.4940047961630696,
      "grad_norm": 0.8599410057067871,
      "learning_rate": 1.4951923076923078e-05,
      "loss": 0.8578,
      "step": 730
    },
    {
      "epoch": 3.5419664268585134,
      "grad_norm": 0.721449613571167,
      "learning_rate": 1.4471153846153848e-05,
      "loss": 0.8602,
      "step": 740
    },
    {
      "epoch": 3.5899280575539567,
      "grad_norm": 0.7101123929023743,
      "learning_rate": 1.3990384615384617e-05,
      "loss": 0.8771,
      "step": 750
    },
    {
      "epoch": 3.6378896882494005,
      "grad_norm": 0.6829865574836731,
      "learning_rate": 1.3509615384615384e-05,
      "loss": 0.8601,
      "step": 760
    },
    {
      "epoch": 3.6858513189448443,
      "grad_norm": 0.63904869556427,
      "learning_rate": 1.3028846153846156e-05,
      "loss": 0.8546,
      "step": 770
    },
    {
      "epoch": 3.7338129496402876,
      "grad_norm": 0.7969698905944824,
      "learning_rate": 1.2548076923076923e-05,
      "loss": 0.849,
      "step": 780
    },
    {
      "epoch": 3.7817745803357314,
      "grad_norm": 0.7104902863502502,
      "learning_rate": 1.2067307692307693e-05,
      "loss": 0.8463,
      "step": 790
    },
    {
      "epoch": 3.8297362110311752,
      "grad_norm": 0.6547456383705139,
      "learning_rate": 1.1586538461538462e-05,
      "loss": 0.8404,
      "step": 800
    },
    {
      "epoch": 3.8776978417266186,
      "grad_norm": 0.8517594933509827,
      "learning_rate": 1.110576923076923e-05,
      "loss": 0.8382,
      "step": 810
    },
    {
      "epoch": 3.9256594724220624,
      "grad_norm": 0.949693500995636,
      "learning_rate": 1.0625e-05,
      "loss": 0.8521,
      "step": 820
    },
    {
      "epoch": 3.973621103117506,
      "grad_norm": 0.7562344670295715,
      "learning_rate": 1.014423076923077e-05,
      "loss": 0.8417,
      "step": 830
    },
    {
      "epoch": 4.019184652278177,
      "grad_norm": 0.9756717085838318,
      "learning_rate": 9.66346153846154e-06,
      "loss": 0.7987,
      "step": 840
    },
    {
      "epoch": 4.067146282973621,
      "grad_norm": 0.6702218651771545,
      "learning_rate": 9.182692307692308e-06,
      "loss": 0.8453,
      "step": 850
    },
    {
      "epoch": 4.115107913669065,
      "grad_norm": 0.6805027723312378,
      "learning_rate": 8.701923076923077e-06,
      "loss": 0.853,
      "step": 860
    },
    {
      "epoch": 4.163069544364508,
      "grad_norm": 0.635204017162323,
      "learning_rate": 8.221153846153847e-06,
      "loss": 0.8262,
      "step": 870
    },
    {
      "epoch": 4.211031175059952,
      "grad_norm": 0.7104341983795166,
      "learning_rate": 7.740384615384616e-06,
      "loss": 0.8236,
      "step": 880
    },
    {
      "epoch": 4.258992805755396,
      "grad_norm": 0.6361725330352783,
      "learning_rate": 7.259615384615385e-06,
      "loss": 0.828,
      "step": 890
    },
    {
      "epoch": 4.306954436450839,
      "grad_norm": 0.7706806063652039,
      "learning_rate": 6.778846153846154e-06,
      "loss": 0.8248,
      "step": 900
    },
    {
      "epoch": 4.3549160671462825,
      "grad_norm": 0.9117843508720398,
      "learning_rate": 6.2980769230769234e-06,
      "loss": 0.8171,
      "step": 910
    },
    {
      "epoch": 4.402877697841727,
      "grad_norm": 1.0552527904510498,
      "learning_rate": 5.817307692307693e-06,
      "loss": 0.8418,
      "step": 920
    },
    {
      "epoch": 4.45083932853717,
      "grad_norm": 0.6919039487838745,
      "learning_rate": 5.3365384615384615e-06,
      "loss": 0.8196,
      "step": 930
    },
    {
      "epoch": 4.498800959232614,
      "grad_norm": 0.7654889225959778,
      "learning_rate": 4.855769230769231e-06,
      "loss": 0.8274,
      "step": 940
    },
    {
      "epoch": 4.546762589928058,
      "grad_norm": 0.6399465799331665,
      "learning_rate": 4.375e-06,
      "loss": 0.8181,
      "step": 950
    },
    {
      "epoch": 4.594724220623501,
      "grad_norm": 0.6123225688934326,
      "learning_rate": 3.894230769230769e-06,
      "loss": 0.8319,
      "step": 960
    },
    {
      "epoch": 4.642685851318944,
      "grad_norm": 1.0336308479309082,
      "learning_rate": 3.413461538461538e-06,
      "loss": 0.8265,
      "step": 970
    },
    {
      "epoch": 4.690647482014389,
      "grad_norm": 0.7684397101402283,
      "learning_rate": 2.932692307692308e-06,
      "loss": 0.8323,
      "step": 980
    },
    {
      "epoch": 4.738609112709832,
      "grad_norm": 0.7326937317848206,
      "learning_rate": 2.451923076923077e-06,
      "loss": 0.8184,
      "step": 990
    },
    {
      "epoch": 4.786570743405276,
      "grad_norm": 0.6512711644172668,
      "learning_rate": 1.971153846153846e-06,
      "loss": 0.8227,
      "step": 1000
    },
    {
      "epoch": 4.83453237410072,
      "grad_norm": 0.676946222782135,
      "learning_rate": 1.4903846153846156e-06,
      "loss": 0.8272,
      "step": 1010
    },
    {
      "epoch": 4.882494004796163,
      "grad_norm": 0.610289990901947,
      "learning_rate": 1.0096153846153846e-06,
      "loss": 0.8268,
      "step": 1020
    },
    {
      "epoch": 4.930455635491606,
      "grad_norm": 0.6602169275283813,
      "learning_rate": 5.288461538461539e-07,
      "loss": 0.8162,
      "step": 1030
    },
    {
      "epoch": 4.9784172661870505,
      "grad_norm": 0.7856941223144531,
      "learning_rate": 4.8076923076923085e-08,
      "loss": 0.814,
      "step": 1040
    }
  ],
  "logging_steps": 10,
  "max_steps": 1040,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.8725785292503096e+20,
  "train_batch_size": 36,
  "trial_name": null,
  "trial_params": null
}
